# RTX AI Toolkit

RTX AI Toolkit provides a seamless developer workflow designed for fine-tuning AI models, including Large Language Models (LLMs), directly on your local Windows RTX PC. This streamlined process supports not only local deployment but also the ability to deploy as cloud endpoints, offering flexibility and scalability to meet diverse deployment needs.

<illustration>

### Workflow Overview

The RTX AI Toolkit workflow is structured into two main phases: the Model Building Phase and the Deployment Phase. Each phase is tailored to guide you through the necessary steps to effectively fine-tune and deploy your AI models.

Currently, we support an end-to-end workflow for fine-tuning LLMs using PEFT (Parameter Efficient Fine-Tuning) techniques on your RTX PC and deploying using NVIDIA AI Inference Manager (NvAIM) SDK, ONNX-Runtime, or as NIM endpoints in the cloud.



## Getting started


### Model Building
RTX AI Toolkit leverages NVIDIA Workbench for 
