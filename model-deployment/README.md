

This directory contains instructions to deploy the model generated by RTX AI Toolkit's fine-tuning process for deployment for the following workflows.


As seen in the LLM Finetuning tutorial, the LlamaFactory app supports export of either LoRA adapters or merged HF checkpoints. Here, we will see how to optimize both these options for deployment across a variety of platforms.

| Platform | LoRA Adapter Support | Merged checkpoint Support |
| -------- | :------------------: | :-----------------------: |
| TensorRT-LLM | |  ✅ |
| llama.cpp |    ✅ |   ✅ |
| vLLM |    ✅ |   ✅ |
| ONNX Runtime |     |   ✅ |
| NIMs|    ✅ |   ✅ |