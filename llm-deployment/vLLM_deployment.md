

## vLLM Deployment

vLLM provides an HTTP server that implements OpenAIâ€™s Completions and Chat API.

In this guide, we will see how to deploy a vLLM OpenAI-compatible microservice.

### 0. Pre-requisites

A Linux instance with NVIDIA GPU and nvidia-container-toolkit:


### 1. Build vLLM Docker 

See all supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).


### 2. Deploy microservice


