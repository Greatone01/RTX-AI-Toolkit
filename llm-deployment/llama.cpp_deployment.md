
# llama.cpp deployment 
This guide demonstrates converting 

Convert model checkpoints generated by RTX AI Toolkit for deployment using the llama.cpp library. 

At the end 

## 0. Pre-requisites
Build llama.cpp with CUDA acceleration by following the instructions [here](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#build). Ensure you have the correct pre-requisites.

Clone and build the llama.cpp repo
<pre>
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
cmake -B build -DLLAMA_CUDA=ON
cmake --build build --config Release
python3 -m pip install -r requirements.txt
</pre>

## 1. HF checkpoint to GGUF conversion
To run inference, first, let's the HF checkpoint generated by LlamaFactory in the GGUF model format used by llama.cpp. Then quantize the model to desired quantization level. 

<pre>
python convert-hf-to-gguf.py <model_dir> --outfile <output_dir>
</pre>

For example:
<pre>
python convert-hf-to-gguf.py C:\models\codealpaca-merged --outfile C:\models\codealpaca.gguf
</pre>

Quantize down to Q4:
<pre>
cd build\bin\Debug
quantize.exe C:\models\codealpaca.gguf C:\models\codealpaca_q4.gguf Q4_K_M
</pre>

## 2. Inference using Python API

### Setup llama-cpp-python
<pre>
set CMAKE_ARGS=-DLLAMA_CUBLAS=on
set FORCE_CMAKE=1
pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir
</pre>

<pre>
from llama_cpp import Llama

llm = Llama(
      model_path="C:\models\llama-model.gguf",
      n_gpu_layers=-1, #Too use GPU acceleration
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
)
output = llm(
      "Q: Name the planets in the solar system? A: ", # Prompt
      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
      stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
      echo=True # Echo the prompt back in the output
) # Generate a completion, can also call create_completion
print(output)
</pre>



## 2. Convert checkpoint to GGML